{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd6d6195-ee60-4a38-873d-0cd41872706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### spark session\n",
    "\n",
    "# Ref : https://github.com/subhamkharwal/pyspark-zero-to-hero/blob/master/05_sort_union_aggregation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c7a731-a621-4a75-bb68-f0d0841dbcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 23:17:52 WARN Utils: Your hostname, prashan resolves to a loopback address: 127.0.1.1; using 192.168.29.201 instead (on interface wlp0s20f3)\n",
      "25/06/04 23:17:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/04 23:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Spark Session\")\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c42287a-f2b9-4850-b4ff-b8050650a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ac06c40d180>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #print session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a006002-8a29-4525-a150-71c91849f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe\n",
    "data = [[1, \"ABC\", 2000], [2, \"DEF\", 4000], [3, \"GGGG\", 3000], [4, 'HHHH', 7000], [5, 'IIIII', 4500]]\n",
    "df = spark.createDataFrame(data=data, schema=\"id int, name string, salary int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9596ce7e-14b9-4f5e-bad4-5782a545e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a68d94f0-9a12-42f8-9c81-1fb791735d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  4|HHHH|  7000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter\n",
    "df.where(\"salary > 5000\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4217b088-5655-4a01-b8b1-f10594c8ddd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() # num of partti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcb17df2-d751-46f8-b543-b08ab0d4a35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/prashant/pivot/personal/tech/pyspark_samples/test.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#action\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/prashant/pivot/personal/tech/pyspark_samples/test.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df.write.format('csv').save(\"test.csv\") #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a63e6c3-8d20-4176-baa8-e4ecf5472ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a674bfa4fa0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Active Session and Rename\n",
    "\n",
    "spark_new = spark.getActiveSession()\n",
    "spark_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6245f8fe-a782-45a9-becf-5fde2b292b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "277ee3cd-9ea5-4e7a-a8fd-fd34b71e541d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('salary', IntegerType(), True)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdca758e-bc33-4c25-b8f8-ac1aa512e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a91d7c3f-8468-4872-8f46-68640ea180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark_new.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5f0c37-965e-4e65-b305-5ec71ff16cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2059ee3a-e4e4-408a-9624-8052b2384883",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"id int, name string, salary int\"\n",
    "df1 = spark_new.createDataFrame(data=data, schema=schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df1df41-2e52-41f7-afec-71861f801c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37aaa6f8-1087-4c20-8357-e1769568bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df1.select(col(\"id\"), df1.name, expr(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69c325e6-3729-48ba-90cc-40adbafc24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.selectExpr(\"cast(id as int) as id\", \"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ca9c34b-0d62-4254-a92b-582738ab4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"tax\", col(\"salary\") * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac45b677-d1e8-422e-9e87-736153a6557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary|  taxx|\n",
      "+---+-----+------+------+\n",
      "|  1|  ABC|  2000| 400.0|\n",
      "|  2|  DEF|  4000| 800.0|\n",
      "|  3| GGGG|  3000| 600.0|\n",
      "|  4| HHHH|  7000|1400.0|\n",
      "|  5|IIIII|  4500| 900.0|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumnRenamed(\"tax\", \"taxx\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7db3e2cb-413d-4634-abdf-7dfe5e9dcde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.drop(\"tax\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8aae17c-a355-4a23-a679-8d71193fa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "columns = {\n",
    "    \"col1\" : lit(1),\n",
    "    \"tax\" : expr(\"salary\") * 0.2,\n",
    "    \"col2\" : lit(\"col2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2bad62f-275b-4c6d-945a-52368a14bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+------+----+\n",
      "| id| name|salary| bonus|col1|   tax|col2|\n",
      "+---+-----+------+------+----+------+----+\n",
      "|  1|  ABC|  2000| 400.0|   1| 400.0|col2|\n",
      "|  2|  DEF|  4000| 800.0|   1| 800.0|col2|\n",
      "|  3| GGGG|  3000| 600.0|   1| 600.0|col2|\n",
      "|  4| HHHH|  7000|1050.0|   1|1400.0|col2|\n",
      "|  5|IIIII|  4500| 675.0|   1| 900.0|col2|\n",
      "+---+-----+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumns(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b3c1a9f-9a4b-4dc5-b9dd-01b8eb5fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2a56313-559a-4dbd-a756-ea3c49d9cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"bonus\", \n",
    "               when(col(\"salary\") <= 4000, col(\"salary\") * 0.2)\n",
    "               .when(col(\"salary\")>4000, col(\"salary\") * 0.15)\n",
    "               .otherwise(col(\"salary\")*1)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e35be6e3-4b07-448b-81d5-936819c413cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary| bonus|\n",
      "+---+-----+------+------+\n",
      "|  1|  ABC|  2000| 400.0|\n",
      "|  2|  DEF|  4000| 800.0|\n",
      "|  3| GGGG|  3000| 600.0|\n",
      "|  4| HHHH|  7000|1050.0|\n",
      "|  5|IIIII|  4500| 675.0|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c617f904-2ac5-4687-ab8e-53a66ff05f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+\n",
      "| id| name|salary| bonus|   tax|\n",
      "+---+-----+------+------+------+\n",
      "|  1|  ABC|  2000| 400.0| 200.0|\n",
      "|  2|  DEF|  4000| 800.0| 400.0|\n",
      "|  3| GGGG|  3000| 600.0| 300.0|\n",
      "|  4| HHHH|  7000|1050.0|1400.0|\n",
      "|  5|IIIII|  4500| 675.0| 900.0|\n",
      "+---+-----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"tax\", expr(\"case when salary <= 4000 then (salary * 0.1) when salary > 4000 then (salary * 0.2) else 0 end\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f86677-3d7c-4a59-b160-51d9044ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION \n",
    "'''\n",
    "Column names, datatypes and sequence of columns of both the dataframes should be same.\n",
    "union all - keeps duplicate rows\n",
    "union - keeps distinct records\n",
    "unionByName - works with diff column seq but number of cols and datatype should be same\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e35213-1135-41f3-b1b2-a969834aa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data_1 = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"]\n",
    "]\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32bf011-f200-4e49-89a8-df01e82514a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emp_df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark_new\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(data\u001b[38;5;241m=\u001b[39memp_data_1, schema \u001b[38;5;241m=\u001b[39m emp_schema)\n\u001b[1;32m      2\u001b[0m emp_df2 \u001b[38;5;241m=\u001b[39m spark_new\u001b[38;5;241m.\u001b[39mcreateDataFrame(data\u001b[38;5;241m=\u001b[39memp_data_2, schema \u001b[38;5;241m=\u001b[39m emp_schema)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark_new' is not defined"
     ]
    }
   ],
   "source": [
    "emp_df1 = spark_new.createDataFrame(data=emp_data_1, schema = emp_schema)\n",
    "emp_df2 = spark_new.createDataFrame(data=emp_data_2, schema = emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfeed930-6a3c-42b9-af7c-e60bdfde1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = emp_df1.union(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "abe7b43a-7f7b-4930-865a-412f47e2167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3683b42a-30f5-4931-b564-2110f2450938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "emp_df_new = emp_df.withColumn(\"new_gender\", coalesce(\"gender\", lit(\"O\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "758835be-3953-4d16-8a68-8dc371509a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|      Male|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|    Female|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|      Male|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|    Female|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|      Male|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|    Female|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|      Male|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|    Female|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|      Male|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|    Female|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|      Male|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|    Female|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|      Male|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|    Female|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|      Male|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|    Female|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|      Male|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|          |\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|      Male|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|    Female|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09da2c1a-b88b-4209-bd7c-32cd408be235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|department_id|count(employee_id)|\n",
      "+-------------+------------------+\n",
      "|          101|                 3|\n",
      "|          102|                 4|\n",
      "|          103|                 4|\n",
      "|          104|                 3|\n",
      "|          105|                 2|\n",
      "|          106|                 2|\n",
      "|          107|                 2|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "emp_df_new.groupBy(\"department_id\").agg(count(\"employee_id\")).alias(\"employee_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d70503dd-8303-4f54-b752-5ad9bf49234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|department_id|\n",
      "+-------------+\n",
      "|          101|\n",
      "|          102|\n",
      "|          103|\n",
      "|          104|\n",
      "|          105|\n",
      "|          106|\n",
      "|          107|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct / unique \n",
    "emp_df = emp_df.distinct()\n",
    "emp_df.select(\"department_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "602c7485-0341-485c-8d78-cb9c4c47e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|max_salary|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|     70000|\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     70000|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|     70000|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|     55000|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|     55000|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|     55000|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|     55000|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|     62000|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     62000|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|     62000|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|     62000|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|     65000|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|     65000|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|     65000|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|     57000|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|     57000|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     75000|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|     75000|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|     49000|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|     49000|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, desc, col\n",
    "\n",
    "window_func = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "max_func = max(col(\"salary\")).over(window_func)\n",
    "\n",
    "emp_df.withColumn(\"max_salary\", max_func).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9a84ad8d-81cb-41ca-9edc-6864244caf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date| rn|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|        001|          101|   John Doe| 30|  Male| 50000|2015-01-01|  2|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|  2|\n",
      "|        005|          103|  Jack Chan| 40|  Male| 60000|2013-04-01|  2|\n",
      "|        018|          104|  Nancy Liu| 29|      | 50000|2017-06-01|  2|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|  2|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|  2|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|  2|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, desc\n",
    "\n",
    "window_func = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "rn_func = row_number().over(window_func)\n",
    "emp_df.withColumn(\"rn\", rn_func).where(\"rn==2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "30024bf1-9b5d-48d3-92c9-ddbcbc947aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date| rn|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|        001|          101|   John Doe| 30|  Male| 50000|2015-01-01|  2|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|  2|\n",
      "|        005|          103|  Jack Chan| 40|  Male| 60000|2013-04-01|  2|\n",
      "|        018|          104|  Nancy Liu| 29|      | 50000|2017-06-01|  2|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|  2|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|  2|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|  2|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternative to window function is expr\n",
    "\n",
    "emp_df.withColumn(\"rn\", expr(\"row_number() over(partition by department_id order by salary desc)\")).where(\"rn == 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "85cda258-4307-4dec-83d1-874b35f4a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition vs coalesce\n",
    "'''\n",
    "repartition : \n",
    "can increase or decrease num of partitions, \n",
    "full shuffle of data\n",
    "coalesce : \n",
    "can only decrease num of partitions, \n",
    "No full shuffle of data merges adjacent partitions\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be43071-6afa-47f1-89b0-4aa8684e42e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "966b8fb3-e472-45f5-9f00-15e48056161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be26ee19-85b8-43f8-9a39-257b5d8c90ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_id: string, department_id: string, name: string, age: string, gender: string, salary: string, hire_date: string]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a5a9318a-32fc-4c04-ace4-757a533da7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d5af5075-5d1b-4bf5-b0c1-e0e3bff18c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           0|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           0|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           0|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           0|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           0|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           0|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           0|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           0|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           0|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           0|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           0|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           0|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           0|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           0|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           0|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|           0|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           0|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           0|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9ae1b5e-9b86-4b45-a327-35fad7710819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           0|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           0|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           0|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           0|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           0|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           0|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           1|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           1|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           2|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           2|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           2|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           2|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|           2|\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           3|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           3|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           3|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           3|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           3|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           3|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           3|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.repartition(4, \"department_id\").withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6688d-546d-4b2b-99eb-1eb2445577da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cascade join\n",
    "'''\n",
    "emp.alias(\"e\").join(dept.alias(d), \n",
    "                    how = 'left_outer', \n",
    "                    on = (emp.department_id == dept.department_id) & ((emp.department_id == \"101\") | (emp.department_id == \"102\"))\n",
    "                   )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ee30fd6e-b98b-44b2-b769-700ec170a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.write.format(\"csv\").save(\"employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bb6abe-6a9c-4d8e-a38c-171bfda2ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates 1 job in bg as it reads only 1 record to read metadata \n",
    "emp_df = spark.read.format(\"csv\").load(\"employee.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bbf77c-831e-4e26-8596-a9055775d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_c0', StringType(), True), StructField('_c1', StringType(), True), StructField('_c2', StringType(), True), StructField('_c3', StringType(), True), StructField('_c4', StringType(), True), StructField('_c5', StringType(), True), StructField('_c6', StringType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d39b497-a165-424f-b8f6-ab4bddd5acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates 1 job in bg as it reads only 1 record to read metadata \n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).load(\"employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237e8ef8-ac89-42de-b795-829d09f007ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates 2 jobs in bg 1 to read header and 2nd to load entire data to get metadata\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9492103d-5926-43f1-96b3-9fe48db35479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('department_id', IntegerType(), True), StructField('_c2', StringType(), True), StructField('age', IntegerType(), True), StructField('gender', StringType(), True), StructField('salary', IntegerType(), True), StructField('joining_date', DateType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498f3f3b-85b7-49e9-9132-3149ced3c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates 0 job as metadata is already provided as schema\n",
    "schema_str = \"id int, department_id int, name string, age int, gender string, salary int, joining_date date\"\n",
    "emp_df = spark.read.format(\"csv\").schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612ae7be-b50e-489e-aa30-646518c9eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91326b7-701b-449c-8bf7-a44b4f986a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+\n",
      "| id|department_id|          _c2|age|gender|salary|joining_date|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "|  1|          101|     John Doe| 30|  Male| 50000|  2015-01-01|\n",
      "|  2|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|\n",
      "|  3|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|\n",
      "|  5|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|\n",
      "|  4|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|\n",
      "|  6|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|\n",
      "|  7|          101|James Johnson| 42|  Male| 70000|  2012-03-15|\n",
      "|  8|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|\n",
      "| 10|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|\n",
      "|  9|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|\n",
      "| 11|          104|   David Park| 38|  Male| 65000|  2015-11-01|\n",
      "| 12|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|\n",
      "| 13|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|\n",
      "| 15|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|\n",
      "| 14|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|\n",
      "| 16|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|\n",
      "| 17|          105|  George Wang| 34|  Male| 57000|  2016-03-15|\n",
      "| 18|          104|    Nancy Liu| 29|  NULL| 50000|  2017-06-01|\n",
      "| 19|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|\n",
      "| 20|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 23:07:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: id, department_id, , age, gender, salary, joining_date\n",
      " Schema: id, department_id, _c2, age, gender, salary, joining_date\n",
      "Expected: _c2 but found: \n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/employee_data.csv\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8eaee2-3918-4d15-8c74-91708637281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PERMISSIVE mode - mark corrupt record under _corrupt_record column\n",
    "FAIL\n",
    "'''\n",
    "schema_str = \"id int, department_id int, name string, age int, gender string, salary int, joining_date date, _corrupt_record string\"\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"PERMISSIVE\").schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a27317b-932e-401d-afae-4ab574198b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+---------------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date|_corrupt_record|\n",
      "+---+-------------+-------------+---+------+------+------------+---------------+\n",
      "|  1|          101|     John Doe| 30|  Male| 50000|  2015-01-01|           NULL|\n",
      "|  2|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|           NULL|\n",
      "|  3|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|           NULL|\n",
      "|  5|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|           NULL|\n",
      "|  4|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|           NULL|\n",
      "|  6|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|           NULL|\n",
      "|  7|          101|James Johnson| 42|  Male| 70000|  2012-03-15|           NULL|\n",
      "|  8|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|           NULL|\n",
      "| 10|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|           NULL|\n",
      "|  9|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|           NULL|\n",
      "| 11|          104|   David Park| 38|  Male| 65000|  2015-11-01|           NULL|\n",
      "| 12|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|           NULL|\n",
      "| 13|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|           NULL|\n",
      "| 15|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|           NULL|\n",
      "| 14|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|           NULL|\n",
      "| 16|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|           NULL|\n",
      "| 17|          105|  George Wang| 34|  Male| 57000|  2016-03-15|           NULL|\n",
      "| 18|          104|    Nancy Liu| 29|  NULL| 50000|  2017-06-01|           NULL|\n",
      "| 19|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|           NULL|\n",
      "| 20|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|           NULL|\n",
      "+---+-------------+-------------+---+------+------+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 23:52:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: id, department_id, , age, gender, salary, joining_date\n",
      " Schema: id, department_id, name, age, gender, salary, joining_date\n",
      "Expected: name but found: \n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/employee_data.csv\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200c9e6b-d7bb-4600-b399-a05b70630535",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' use different name to mark bad_record '''\n",
    "schema_str = \"id int, department_id int, name string, age int, gender string, salary int, joining_date date, bad_record string\"\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"columnNameOfCorruptRecord\", \"bad_record\").schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ed8f43-81b9-444b-89bc-961829303f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "|  1|          101|     John Doe| 30|  Male| 50000|  2015-01-01|\n",
      "|  2|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|\n",
      "|  3|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|\n",
      "|  5|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|\n",
      "|  4|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|\n",
      "|  6|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|\n",
      "|  7|          101|James Johnson| 42|  Male| 70000|  2012-03-15|\n",
      "|  8|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|\n",
      "| 10|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|\n",
      "|  9|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|\n",
      "| 11|          104|   David Park| 38|  Male| 65000|  2015-11-01|\n",
      "| 12|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|\n",
      "| 13|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|\n",
      "| 15|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|\n",
      "| 14|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|\n",
      "| 16|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|\n",
      "| 17|          105|  George Wang| 34|  Male| 57000|  2016-03-15|\n",
      "| 18|          104|    Nancy Liu| 29|  NULL| 50000|  2017-06-01|\n",
      "| 19|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|\n",
      "| 20|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 13:37:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: id, department_id, , age, gender, salary, joining_date\n",
      " Schema: id, department_id, name, age, gender, salary, joining_date\n",
      "Expected: name but found: \n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/employee_data.csv\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a74a3e8-ee9f-49ba-a3b2-8e8778329b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DROPMALFORMED  drop corrupt records '''\n",
    "\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f68944-3de9-4cc1-a7a4-6ffc4b6b3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FAILFAST : fail data read if any corrupt record is found '''\n",
    "schema_str = \"id int, department_id int, name string, age int, gender string, salary int, joining_date date\"\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(schema_str).load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85513449-b954-489d-bbf7-9986a7c6bbfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 06:07:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: id, department_id, , age, gender, salary, joining_date\n",
      " Schema: id, department_id, name, age, gender, salary, joining_date\n",
      "Expected: name but found: \n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/employee_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "|  1|          101|     John Doe| 30|  Male| 50000|  2015-01-01|\n",
      "|  2|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|\n",
      "|  3|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|\n",
      "|  5|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|\n",
      "|  4|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|\n",
      "|  6|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|\n",
      "|  7|          101|James Johnson| 42|  Male| 70000|  2012-03-15|\n",
      "|  8|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|\n",
      "| 10|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|\n",
      "|  9|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|\n",
      "| 11|          104|   David Park| 38|  Male| 65000|  2015-11-01|\n",
      "| 12|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|\n",
      "| 13|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|\n",
      "| 15|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|\n",
      "| 14|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|\n",
      "| 16|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|\n",
      "| 17|          105|  George Wang| 34|  Male| 57000|  2016-03-15|\n",
      "| 18|          104|    Nancy Liu| 29|  NULL| 50000|  2017-06-01|\n",
      "| 19|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|\n",
      "| 20|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|\n",
      "+---+-------------+-------------+---+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28487caa-8d72-4504-8ed9-47406829ec5a",
   "metadata": {},
   "source": [
    "'''\n",
    "Read json file \n",
    "from_json - is used to parse string to json format by providing schema\n",
    "to_json - convert json to string format\n",
    "'''\n",
    "schema_str = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amounnt double, item_id int>>\"\n",
    "# df_expanded = df.withColumn(\"parsed_data\", from_json(df.value, schema_str))\n",
    "# df1 = df_expanded.select(\"parsed_data.*\")\n",
    "# df2 = df1.withColumn(\"exploded_line_items\", explode(\"order_line_items\"))\n",
    "# df3 = df2.select(\"col1, col2. col3, col4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6659b6-7d35-4132-8a2a-edd4304bc553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3adea65-a2d7-438b-a91f-db3d15ec0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data_1 = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"]\n",
    "]\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "emp_data_1.extend(emp_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d21756-1465-48c4-8fdb-1437f957634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"id string, department_id string, name string, age string, gender string, salary string, joining_date string\"\n",
    "emp_df = spark.createDataFrame(data=emp_data_1, schema=schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7efa6ee-eb10-4cda-a53d-7f05a3785bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fec973-01f2-400e-8eb2-8150b36afcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''\n",
    "8 files got created in the emp_data.csv folder, as each partition wrote its file independently at destination location'''\n",
    "emp_df.write.format(\"csv\").save(\"emp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e72239-9793-4e49-bae5-2b9839fe9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8 cores holds 8 partitions ie. dataframe is divided in 8 parts and then groupby is applied \n",
    "each core applies groupby clause for its partitioned data and writes to folders partitioned by department_id\n",
    "\n",
    "department_id=101\n",
    "department_id=102\n",
    ".\n",
    ".\n",
    "department_id=108   each folder may have multiple files \n",
    "if 1 core has multiple department data then that core would write its data to multiple folders\n",
    "'''\n",
    "emp_df.write.format(\"csv\").option(\"header\", True).partitionBy(\"department_id\").save(\"test_partitioned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77d52b-4334-4327-bc45-41f600a057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write data has 4 modes\n",
    "append : add files to location\n",
    "overwrite : overwrite files even if data is present at same location\n",
    "ignore : ignore if data is available at the same location\n",
    "error : raise an error if data is present at the destination location\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a683d-38cd-49b6-9ec8-536a7cc2a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How Spark executes a job in cluster \n",
    "2 types of deployment :\n",
    "1. client mode : standalone mode (spark cluster mode), yarn mode, mesos, kubernetes\n",
    "2. cluster mode : where driver is part of cluster \n",
    "\n",
    "\n",
    "cluster = 1 driver (spark session) + 1 resource manager (cluster mannager) + 2 nodes\n",
    "\n",
    "step 1 : driver will share the request to resource manager to create resources 4 executors (JVM), 2 cores each = 8 cores in total\n",
    "step 2 : resource manager creates resources\n",
    "step 3 : resource manager will share req info\n",
    "step 4 : driver will copy python code to all the executors \n",
    "step 5 : driver will assign task to each core ie 8 task, 1/core\n",
    "step 6 : once executors completes their task they share the result to driver\n",
    "step 7 : driver will inform resource manager to terminate the resources\n",
    "step 8 : resource manager will destroy the executors which it has created\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07ee37-86f6-4250-b983-2e2e9d5107d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UDF :\n",
    "\n",
    "driver +  2 nodes (2 jvm each)\n",
    "\n",
    "driver : copies python udf code to each node \n",
    "\n",
    "Node : each node creates separate python process which is outside of JVM\n",
    "JVM does not have control over this python process\n",
    "JVM does data serialization and submits data to python process\n",
    "Python processes data row by row and submits result to JVM \n",
    "JVM does deserialization and shares result with driver (master)\n",
    "\n",
    "Disadvantage of UDF : \n",
    "1. serialization - deserialization\n",
    "2. python process \n",
    "3. data row by row\n",
    "\n",
    "Solution : \n",
    "1. Use Higher Order Functions\n",
    "2. Use Java or Scala UDF which can be run inside JVM and can be called from python itself\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6426a209-7a9a-4e75-92ce-c2cf8bcd54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus(salary):\n",
    "    return int(salary) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a1622c1-4115-4744-ad8a-2a3adf03a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "bonus_udf = udf(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2b61094-7dd2-403e-af06-2f0e1f08d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- joining_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4990ac7e-6f40-4cef-8dd3-5206975c96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date| bonus|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "|001|          101|     John Doe| 30|  Male| 50000|  2015-01-01|5000.0|\n",
      "|002|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|4500.0|\n",
      "|003|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|5500.0|\n",
      "|004|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|4800.0|\n",
      "|005|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|6000.0|\n",
      "|006|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|5200.0|\n",
      "|007|          101|James Johnson| 42|  Male| 70000|  2012-03-15|7000.0|\n",
      "|008|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|5100.0|\n",
      "|009|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|5800.0|\n",
      "|010|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|4700.0|\n",
      "|011|          104|   David Park| 38|  Male| 65000|  2015-11-01|6500.0|\n",
      "|012|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|5400.0|\n",
      "|013|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|7500.0|\n",
      "|014|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|4600.0|\n",
      "|015|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|6300.0|\n",
      "|016|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|4900.0|\n",
      "|017|          105|  George Wang| 34|  Male| 57000|  2016-03-15|5700.0|\n",
      "|018|          104|    Nancy Liu| 29|      | 50000|  2017-06-01|5000.0|\n",
      "|019|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|6200.0|\n",
      "|020|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|5300.0|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"bonus\", bonus_udf(\"salary\")).show() #creates python process on worker node and process data row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a7c69-9979-4551-8c9e-e568abd16da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.withColumn(\"bonus\", bonus_udf(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d2c2ca-7b07-40f9-a816-4beab3b32dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 15:29:13 WARN SimpleFunctionRegistry: The function bonus_sql_udf replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.bonus(salary)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ee06e65-4d69-49bc-9064-fb3bcf0db89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date| bonus|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "|001|          101|     John Doe| 30|  Male| 50000|  2015-01-01|5000.0|\n",
      "|002|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|4500.0|\n",
      "|003|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|5500.0|\n",
      "|004|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|4800.0|\n",
      "|005|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|6000.0|\n",
      "|006|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|5200.0|\n",
      "|007|          101|James Johnson| 42|  Male| 70000|  2012-03-15|7000.0|\n",
      "|008|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|5100.0|\n",
      "|009|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|5800.0|\n",
      "|010|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|4700.0|\n",
      "|011|          104|   David Park| 38|  Male| 65000|  2015-11-01|6500.0|\n",
      "|012|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|5400.0|\n",
      "|013|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|7500.0|\n",
      "|014|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|4600.0|\n",
      "|015|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|6300.0|\n",
      "|016|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|4900.0|\n",
      "|017|          105|  George Wang| 34|  Male| 57000|  2016-03-15|5700.0|\n",
      "|018|          104|    Nancy Liu| 29|      | 50000|  2017-06-01|5000.0|\n",
      "|019|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|6200.0|\n",
      "|020|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|5300.0|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "emp_df.withColumn(\"bonus\", expr(\"bonus_sql_udf(salary)\")).show() #spark sql udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c7108f-2770-449b-993a-16453659e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "| id|department_id|         name|age|gender|salary|joining_date| bonus|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "|001|          101|     John Doe| 30|  Male| 50000|  2015-01-01|5000.0|\n",
      "|002|          101|   Jane Smith| 25|Female| 45000|  2016-02-15|4500.0|\n",
      "|003|          102|    Bob Brown| 35|  Male| 55000|  2014-05-01|5500.0|\n",
      "|004|          102|    Alice Lee| 28|Female| 48000|  2017-09-30|4800.0|\n",
      "|005|          103|    Jack Chan| 40|  Male| 60000|  2013-04-01|6000.0|\n",
      "|006|          103|    Jill Wong| 32|Female| 52000|  2018-07-01|5200.0|\n",
      "|007|          101|James Johnson| 42|  Male| 70000|  2012-03-15|7000.0|\n",
      "|008|          102|     Kate Kim| 29|Female| 51000|  2019-10-01|5100.0|\n",
      "|009|          103|      Tom Tan| 33|  Male| 58000|  2016-06-01|5800.0|\n",
      "|010|          104|     Lisa Lee| 27|Female| 47000|  2018-08-01|4700.0|\n",
      "|011|          104|   David Park| 38|  Male| 65000|  2015-11-01|6500.0|\n",
      "|012|          105|   Susan Chen| 31|Female| 54000|  2017-02-15|5400.0|\n",
      "|013|          106|    Brian Kim| 45|  Male| 75000|  2011-07-01|7500.0|\n",
      "|014|          107|    Emily Lee| 26|Female| 46000|  2019-01-01|4600.0|\n",
      "|015|          106|  Michael Lee| 37|  Male| 63000|  2014-09-30|6300.0|\n",
      "|016|          107|  Kelly Zhang| 30|Female| 49000|  2018-04-01|4900.0|\n",
      "|017|          105|  George Wang| 34|  Male| 57000|  2016-03-15|5700.0|\n",
      "|018|          104|    Nancy Liu| 29|      | 50000|  2017-06-01|5000.0|\n",
      "|019|          103|  Steven Chen| 36|  Male| 62000|  2015-08-01|6200.0|\n",
      "|020|          102|    Grace Kim| 32|Female| 53000|  2018-11-01|5300.0|\n",
      "+---+-------------+-------------+---+------+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"bonus\", expr(\"salary * 0.1\")).show() #Soln1 : higher order function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d8c2c5-2c4f-4582-b5ef-3f8574cea83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUnderstand DAG, Explain Plans & Spark Shuffle with Tasks\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Understand DAG, Explain Plans & Spark Shuffle with Tasks\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "131afa4b-fc13-47c2-b51a-5bde8b117e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.range(4, 200, 2)     #stage 1--> 8 \n",
    "df_2 = spark.range(2, 200, 4)     #stage 1--> 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f57766fa-2871-48af-86d3-43805cac5cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.rdd.getNumPartitions(), df_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afd3fd41-36d3-4123-a986-c6a3ae93c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_1.repartition(5) #stage 1--> 5\n",
    "df_4 = df_1.repartition(7) #stage 1--> 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e03e85b-6dad-4468-b45e-410acfd73071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.rdd.getNumPartitions(), df_4.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4f1530c-00f1-4aed-963f-7430a7182103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_3.join(df_4, on=\"id\") # stage 1--> 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab9f11be-f8b1-4682-9589-8de5af0c55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df_joined.selectExpr(\"sum(id) as total_sum\") #stage 1--> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9021747d-fb17-4899-bcf1-67d66018f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|total_sum|\n",
      "+---------+\n",
      "|     9898|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sum.show() #total stages = 6, total_tasks = 8 + 8 + 5 + 7 + 200 + 1 = 229 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "425e01dc-1ae7-4463-951d-296ac463c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#271L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=568]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#271L)])\n",
      "         +- Project [id#271L]\n",
      "            +- BroadcastHashJoin [id#271L], [id#279L], Inner, BuildRight, false\n",
      "               :- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=555]\n",
      "               :  +- Range (4, 200, step=2, splits=8)\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=563]\n",
      "                  +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=557]\n",
      "                     +- Range (4, 200, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sum.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03f3dd-72d1-4711-8a3d-c400a2931e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "why shuffle writes data ?\n",
    "because if it writes data to node then if any future task fails then previous stages need not to be executed again\n",
    "it will read that data written (prev stage in which data was written to node) to node and resume the next stages \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10f5216d-f049-4979-bde2-0fa3499f07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = df_sum.union(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de8095f2-935d-4af8-8ec1-d7ba44415747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|total_sum|\n",
      "+---------+\n",
      "|     9898|\n",
      "|       10|\n",
      "|       12|\n",
      "|       46|\n",
      "|       34|\n",
      "|       64|\n",
      "|       90|\n",
      "|       84|\n",
      "|      120|\n",
      "|      136|\n",
      "|      164|\n",
      "|      166|\n",
      "|      182|\n",
      "|      174|\n",
      "|        6|\n",
      "|       50|\n",
      "|       36|\n",
      "|       56|\n",
      "|       98|\n",
      "|       80|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a923129-7460-479f-bab5-64aee0b20e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Union\n",
      "   :- HashAggregate(keys=[], functions=[sum(id#271L)])\n",
      "   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=892]\n",
      "   :     +- HashAggregate(keys=[], functions=[partial_sum(id#271L)])\n",
      "   :        +- Project [id#271L]\n",
      "   :           +- BroadcastHashJoin [id#271L], [id#279L], Inner, BuildRight, false\n",
      "   :              :- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=876]\n",
      "   :              :  +- Range (4, 200, step=2, splits=8)\n",
      "   :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=887]\n",
      "   :                 +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=878]\n",
      "   :                    +- Range (4, 200, step=2, splits=8)\n",
      "   +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=884]\n",
      "      +- Range (4, 200, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432656ea-689d-4d56-a46e-5430deb1deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataframes are abstractions of RDD (Resilient Distributed Data) sparkcore\n",
    "1.SparkSQL and DataFrames\n",
    "2.Pandas API on spark\n",
    "3.Structured streaming\n",
    "4.Machine Learning\n",
    "\n",
    "RDD is recommended when you need to distribtue data physcially by code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "355ba22e-7054-44f4-b727-bb45d5fbcb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.32.120:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x721d643251e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Optimize Shuffle\n",
    "\n",
    "Narrow transformations involves single partitions \n",
    "Wide transformations involves multiple partitions\n",
    "\n",
    "Spark pipeline keep on adding multiple narrow partitions to single pipeline until it encounters wide transformation.\n",
    "and creates again new pipeline with other narrow transformations.\n",
    "When spark pipeline encounters any wide transformation it involves shuffle stage.\n",
    "Hence shuffle stage breaks spark pipeline and creates separate stage.\n",
    "\n",
    "First Pipeline\n",
    "Spark pipeline includes all possible narrow tx and in the end writes shuffle files\n",
    "Shuffles files - are serialized in tungsten binary format (unsafe row) can be read directly in memory heance improving read performance\n",
    "\n",
    "Second Pipeline\n",
    "will read shuffle files in first step and then will be processing remaining narrow transformations.\n",
    "\n",
    "Shuffle files are written to disk and it involves io operation and are transferred to other executors which involves network operation.\n",
    "Which inturn slow downs the process. Hence try to avoid shuffle operation wherever possible.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90a9398e-c12e-42d5-99e9-25791d19ab6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check default parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2af44521-6fc4-4b6b-bb05-01e2329d0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 07:12:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.32.120:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x721d643251e0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Shuffles\")\n",
    "    .master(\"spark://17e348267994:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "342aa8c7-9a9c-4baf-91e6-22f28a712633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03ef15a5-9b33-49e0-837e-7212648b8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and broadcast join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadCastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b46a912a-f64b-475a-ae14-e0f8fc1a56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10M records\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"employee_records.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f39ce1b4-0dc4-4b09-a878-2c2a4c5304bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out avg salary as per dept\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c774be58-c563-479b-ae77-349a05f89e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write data for performance Benchmarking\n",
    "\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1647d19-11b9-4bb8-aec9-769aad737f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Spark Shuffle Partition setting\n",
    "\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef55c4-cbe0-42b3-be1f-1ffa276febb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6239b-07a4-4693-9583-c7df06fc947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id = 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd49253-0de4-4648-957d-cbf931687937",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set shuffle partitions to appropriate value\n",
    "which can avoid overkilling of cpu\n",
    "lower shuffle partitons can cause memory error\n",
    "higher shuffle partition can increase network time and overkill\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3aca13-9fbf-4af6-a59c-e60bc692afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the partitioned data\n",
    "\n",
    "emp_part = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/emp_partitioned.csv/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d508922-37f3-481b-98ca-a3c0d428b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg = emp_part.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373cf3e-13ec-4209-b5b8-912abcbe0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save() #simulate write operation without actually writing data with noop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8bc18-d864-4485-b11c-a2417cb172fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading partitioned data improves shuffle operation\n",
    "\n",
    "Good shuffle - avoid unnecessary shuffle\n",
    "repartition data - reduces shuffle\n",
    "filter data - before aggregation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea38ea-f23b-44e0-94c2-05735d59cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading 752 mb of csv file and caching it \n",
    "\n",
    "cache in spark uses memory and disk\n",
    "cache in rdd uses only memory\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e36da08-76cb-443b-974d-ccfb4c2af031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ac06c40d180>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dynamic Allocation\")\n",
    "    .master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "    .config(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"60s\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0679b93e-30b4-4015-9a50-0973cf401dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(sales_schema).option(\"delimiter\", ',').option(\"header\", True).load(\"pyspark-zero-to-hero/datasets/new_sales-Copy1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb87ba6e-908c-41ff-b3da-4b9346fedffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "|       transacted_at|trx_id|retailer_id|description|amount|city_id|\n",
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "|oid sha256:5ce2c1...|  NULL|       NULL|       NULL|  NULL|   NULL|\n",
      "|      size 787780307|  NULL|       NULL|       NULL|  NULL|   NULL|\n",
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 23:36:24 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 6\n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/pyspark-zero-to-hero/datasets/new_sales-Copy1.csv\n"
     ]
    }
   ],
   "source": [
    "df.show() #whenerver we do show on dataframe everytime it will hit the csv scan to get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b462092-e0c5-4b5e-92cc-60c5cc6cc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----------+-----------+------+-------+\n",
      "|transacted_at|trx_id|retailer_id|description|amount|city_id|\n",
      "+-------------+------+-----------+-----------+------+-------+\n",
      "+-------------+------+-----------+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"amount > 50.0\").show() #everytime it will call csv scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "754e43b8-1eef-46fe-b85f-efb5ecb49659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached = df.cache() #check storage on spark UI it will store data to MEMORY AND DISK as data size is greater than executor memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cfa9c3-0873-4a8f-bf18-dc1bc75916c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 13:08:37 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 6\n",
      "CSV file: file:///home/prashant/pivot/personal/tech/pyspark_samples/pyspark-zero-to-hero/datasets/new_sales.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "|       transacted_at|trx_id|retailer_id|description|amount|city_id|\n",
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "|oid sha256:5ce2c1...|  NULL|       NULL|       NULL|  NULL|   NULL|\n",
      "|      size 787780307|  NULL|       NULL|       NULL|  NULL|   NULL|\n",
      "+--------------------+------+-----------+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cached.count() #count and write operation will load entire dataframe in memory and not the partial records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dfb7a-b636-485d-8b8e-4f637ed9d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached.where(\"amount > 50.0\") #it will not hit the csv scan, it will read data from memory\n",
    "#spark maintains data lineage and identifies whether to hit the csv or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813d446-4b71-437a-84a6-67d6a1638f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached.unpersist() #delete cached data\n",
    "spark.catlog.clearCache() #remove all cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7b421-7141-4c81-9afd-e8a14db63bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached = df.where(\"amount > 100.0\").cache() #partial cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab19955-67a2-4089-8ac3-feaa5fff2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached.where(\"amount > 50\") #this will hit the csv scan as partial data is cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdac1f91-bf9a-4916-a68e-36771743cd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Different storage levels involved in caching\n",
    "cahce: default is MEMORY AND DISK and data is deserialized\n",
    "\n",
    "Persist:\n",
    "MEMORY_ONLY : fits data in MEMORY and data is serialized\n",
    "MEMORY_AND_DISK : fits data in MEMORY and disk and data is serialized\n",
    "MEMORY_ONLY_SER : this option is for scala and java \n",
    "MEMORY_AND_DISK_SER : this option is for scala and java\n",
    "DISK_ONLY : \n",
    "MEMORY_ONLY_2 : fits data in MEMORY and data is serialized and it is replicated twice in executor memory\n",
    "MEMORY_AND_DISK_2 : \n",
    "\n",
    "'''\n",
    "import pyspark\n",
    "df.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09c0641-d3eb-4579-9d9c-73ceb21541d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 07:26:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ac06c40d180>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Distributed Shared variables\n",
    "Broadcast variables\n",
    "Accumulators\n",
    "\n",
    "            Mutable        Readable in executors                Purpose\n",
    "Broadcast\tNo\t            Yes\t                            Share large read-only data\n",
    "Accumulator\tYes (add only)\tNo (write-only in tasks         Aggregate metrics from workers\n",
    "                            and readable in driver)\t\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Distributed Shared Variables\")\n",
    "    .master(\"spark://17e348267994:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d380dff-aeb6-4f13-b712-9c70a8916b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"pyspark-zero-to-hero/datasets/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb6616f5-beef-4ff2-821d-79154f79d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable (Lookup)\n",
    "dept_names = {1 : 'Department 1', \n",
    "              2 : 'Department 2', \n",
    "              3 : 'Department 3', \n",
    "              4 : 'Department 4',\n",
    "              5 : 'Department 5', \n",
    "              6 : 'Department 6', \n",
    "              7 : 'Department 7', \n",
    "              8 : 'Department 8', \n",
    "              9 : 'Department 9', \n",
    "              10 : 'Department 10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c479b840-d0a9-480c-8654-0396fd26263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast the variable\n",
    "\n",
    "broadcast_dept_names = spark.sparkContext.broadcast(dept_names) \n",
    "''' It will broadcast this variable to each of the executor and executor will cache this data \n",
    "Each executor will use this broadcast variable locally and no need to shuffle the data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66dc869-32a3-42e3-bb30-076e86749d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value of the variable\n",
    "broadcast_dept_names.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a412f6b-8e27-405e-a536-b295fc6667ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF to return Department name\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "@udf\n",
    "def get_dept_names(dept_id):\n",
    "    return broadcast_dept_names.value.get(dept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29fd09-128b-4e37-927f-8426080a863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final = emp.withColumn(\"dept_name\", get_dept_names(col(\"department_id\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20248a80-77b2-45bc-92a7-0ac20505acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b6e82-d101-4371-bd07-2cf0fbd2166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total salary of Department 6\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "emp.where(\"department_id = 6\").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show()\n",
    "\n",
    "'''\n",
    "this operation need to bring department 6 data to one executor which will then perform sum operation\n",
    "which involves shuffle\n",
    "\n",
    "To avoid this accumulators are used\n",
    "accumulators will be processed row by row in each of the executors and once the each row of each executor is executed then will get \n",
    "the final value of the accumulator which provides sum\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69f9f0-70d0-41dc-9720-9d6bd830dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulators\n",
    "\n",
    "dept_sal = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4a34-e13f-41b4-a255-ede326610e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use foreach\n",
    "\n",
    "def calculate_salary(department_id, salary):\n",
    "    if department_id == 6:\n",
    "        dept_sal.add(salary)\n",
    "\n",
    "emp.foreach(lambda row : calculate_salary(row.department_id, row.salary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902eab39-dec8-4e30-95aa-5edf5fd514fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View total value\n",
    "\n",
    "dept_sal.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dc8bd-ef15-4e4e-95cf-dc79f1c3df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa317d-163c-4b24-8877-5e4fa1637491",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Joins in Spark\n",
    "\n",
    "1. Shuffle Hash Join - Big table and Small Table\n",
    "2. Sort Merge Join - Big Table Big Table\n",
    "3. Broadcast Hash Join - Small Table Small Table\n",
    "'''oin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed88cf-2a8d-4bc8-b5df-042ab4fc7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sales table : fact table,  City table : dimensions table join on city_id\n",
    "\n",
    "Normal Join In Spark\n",
    "1. Read Data in Partitions              2. Shuffle data         \n",
    "exec  sales.city_id  city.city_id    \n",
    "1      121                              111       1\n",
    "2      334             12               334       34\n",
    "3      15              34               5         5\n",
    "4      2               5                22        2\n",
    "\n",
    "3. Join\n",
    "4. Write / Count\n",
    "\n",
    "\n",
    "Shuffle Hash Join :\n",
    "1. Shuffle data\n",
    "2. Hash small table\n",
    "3. Match hash table with big table\n",
    "4. Join\n",
    "\n",
    "\n",
    "Sort Merge Join :\n",
    "1. Shuffle data\n",
    "2. Sort based on joining key\n",
    "3. Merge\n",
    "\n",
    "Broadcast Hash Join\n",
    "1. small dataset would be broadcasted to each executor\n",
    "2. Joining based on hash key\n",
    "\n",
    "small table default broadcast size : 10 MB can be increased to 8 gb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b6f754-f436-49c7-b23c-8d2662de3457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBucketing Strategy before join\\n\\nUse Murmur hash to create buckets of joining key for both the tables (sales and city table)\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bucketing Strategy before join\n",
    "\n",
    "Use Murmur hash to create buckets of joining key for both the tables (sales and city table)\n",
    "\n",
    "\n",
    "Points To Note : \n",
    "1.Bucketing can only work when we save data as table\n",
    "2.Joining columns different than bucket column, same bucket size - shuffle on both table\n",
    "3.Joining column same, one table in bucket, shuffle on non bucket table\n",
    "4.Joining column same, different bucket size, shuffle on smaller bucket side\n",
    "5.Joining column same, same bucket size, No shuffle (faster join)\n",
    "\n",
    "\n",
    "1.So its very important to choose correct bucket column and bucket size\n",
    "2.Decide effectively on number of buckets, as too any buckets with not enough data can lead to small file issue.\n",
    "3.Datasets are small - you can prefer shuffle hash join\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dbf867-75fc-41db-8d54-bf4acfa4ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 10:31:55 WARN Utils: Your hostname, prashan resolves to a loopback address: 127.0.1.1; using 192.168.29.201 instead (on interface wlp0s20f3)\n",
      "25/06/07 10:31:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/07 10:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/07 10:31:56 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 17e348267994:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Failed to connect to 17e348267994/<unresolved>:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: 17e348267994\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1256)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:990)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "25/06/07 10:32:16 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 17e348267994:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Failed to connect to 17e348267994/<unresolved>:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: 17e348267994\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1256)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:990)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "25/06/07 10:32:36 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 17e348267994:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Failed to connect to 17e348267994/<unresolved>:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: 17e348267994\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1256)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:990)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "25/06/07 10:32:56 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "25/06/07 10:32:56 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "25/06/07 10:32:57 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n\tat org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py:347\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:617\u001b[0m, in \u001b[0;36mSparkSession._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124m        <div>\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[38;5;124m</b></p>\u001b[39m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m--> 617\u001b[0m         catalogImplementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalogImplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    618\u001b[0m         sc_HTML\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39m_repr_html_(),\n\u001b[1;32m    619\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:784\u001b[0m, in \u001b[0;36mSparkSession.conf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runtime configuration interface for Spark.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03mThis is the interface through which the user can get and set all Spark and Hadoop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m'value'\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_conf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m RuntimeConfig(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n\tat org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x714938445180>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Joins\")\n",
    "    .master(\"spark://17e348267994:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c3a06-8994-48a5-b856-c1ef4661d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales CSV Data - 752MB Size ~ 7.2M Records\n",
    "\n",
    "_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c93a80-0039-46c6-be27-26b0dd2c03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdacb78-d533-4977-8df4-15d23c587127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/datasets/employee_records.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8f215-9017-4c83-a27f-744c86924fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/datasets/department_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489679c6-c4f0-4e7f-98a6-9eaaab341648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25a3ff-ba5c-4b2e-9e12-cc9a7074813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"/data/input/datasets/new_sales.csv\")\n",
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"/data/input/datasets/cities.csv\")\n",
    "# Join Data\n",
    "\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc24508-84b4-4905-82a3-0c47b3a587ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Plan\n",
    "#Write Sales and City data in Buckets\n",
    "# Write Sales data in Buckets\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/sales_bucket.csv\").saveAsTable(\"sales_bucket\")\n",
    "# Write City data in Buckets\n",
    "\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/city_bucket.csv\").saveAsTable(\"city_bucket\")\n",
    "# Check tables\n",
    "\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64d68d-7325-4659-87cd-b986908f76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales table\n",
    "\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")\n",
    "# Read City table\n",
    "\n",
    "city_bucket = spark.read.table(\"city_bucket\")\n",
    "# Join datasets\n",
    "\n",
    "df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")\n",
    "# Write dataset\n",
    "\n",
    "df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "df_joined_bucket.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4afb21-cc68-4aaf-be25-546f453ed430",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Static vs Dynamic Allocation in Spark\n",
    "\n",
    "Static Allocation :  Single app is running on cluster\n",
    "\n",
    "    Cluster - 2 worker nodes --> w1 and w2\n",
    "    w1 = 8 core 8 gb\n",
    "    w2 = 8 core 8 gb\n",
    "\n",
    "    total = 16 core 16 gb\n",
    "    \n",
    "    user1 submits app1 on cluster\n",
    "    app1 requirement - 6 core 6gb on each node\n",
    "    total requirement - 12 core 12 gb\n",
    "\n",
    "    In case of static allocation, app1 is running on cluster and eventhough some resources ( 6 cores and 6 gb) \n",
    "    are not in use or app is not using any resource it still holds those resources\n",
    "\n",
    "Dynamic Allocation : multiple user / multiple apps running on same cluster\n",
    "    Whenever any resource is free app running on cluster uses its resources by scaling up \n",
    "    and once execution is completed it scales down or releases resources\n",
    "\n",
    "\n",
    "Dynamic vs Databricks Scale Up Scale Down\n",
    "\n",
    "Dynamic Allocation : executor scale up scale down happens in fixed cluster\n",
    "Databricks : worker node gets added to scale up and removed for scale down \n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77026e-59c4-45d1-afca-0a01c3951798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dynamic Allocation\")\n",
    "    .master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "    .config(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"60s\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"/data/input/new_sales.csv\")\n",
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"/data/input/cities.csv\")\n",
    "# Join Data\n",
    "\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "\n",
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "# Difference between Scale UP in Databricks and Dynamic Allocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08914d6a-ffd4-408a-b35a-80f0fd8ee8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Skewness \n",
    "\n",
    "Data skewness causes spill memory and spill disk issue\n",
    "spill memory --> deserialized data --> 128 mb\n",
    "spill disk --> serialized data --> 70 mb\n",
    "\n",
    "Use salting technique to handle data skewness before joining data\n",
    "\n",
    "emp.dept_id                            dept.dept_id\n",
    "1     0 (randomly assigned)                1     0\n",
    "1     1                                    2     0 \n",
    "1     0                                    3     0  \n",
    "2     1                                    1     1   \n",
    "3     0                                    2     1 \n",
    "3     1                                    3     1 \n",
    "3     0\n",
    "3     1\n",
    "3\n",
    "3\n",
    "3\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fa08e-a793-4e48-8f2b-f60ab7f96062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Skewness and Spillage\")\n",
    "    .master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0488eaa-f513-4bbd-a3b3-caac7c499e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\")\n",
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\")\n",
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "#Explain Plan\n",
    "\n",
    "df_joined.explain()\n",
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae1653-9223-458a-8b31-e5482038fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85ac52-b454-4fd0-943a-c0be16a48d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set shuffle partitions to a lesser number - 16\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)\n",
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 32)\n",
    "\n",
    "# Salt Data Frame to add to department\n",
    "salt_df = spark.range(0, 32)\n",
    "salt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470047bf-3824-496b-b068-421a913b3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salted Employee\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))\n",
    "\n",
    "salted_emp.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2799d-df87-478b-b463-971be3894a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salted Department\n",
    "\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
    "\n",
    "salted_dept.where(\"department_id = 9\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204dba9-5dee-43a3-a820-3d6a22f9ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")\n",
    " \n",
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bcea1-41ac-4830-b96d-ca6abdf4610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AQE \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d40ed-2212-47bf-8ac2-a6e4eddd7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"AQE in Spark\")\n",
    "    .master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609354e7-48f1-4a53-968e-6962497061b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\")\n",
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\")\n",
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "#Explain Plan\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668b570-1659-4a70-8099-1fe208f76951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalescing post-shuffle partitions - remove un-necessary shuffle partitions\n",
    "# Skewed join optimization (balance partitions size) - join smaller partitions and split bigger partition\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "# Fix partition sizes to avoid Skew\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8MB\") #Default value: 64MB\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"10MB\") #Default value: 256MB\n",
    "# Converting sort-merge join to broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "# Join Datasets - without specifying specific broadcast table\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92f6c7-3760-420b-830b-11d379b34c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark SQL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74209fcd-da2e-4fa4-9427-9d2c155475f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark SQL\")\n",
    "    .master(\"local[*]\")\n",
    "    .enableHiveSupport()     #to store metadata to metastore permanent storage \n",
    "    .config(\"spark.sql.warehouse.dir\", \"/data/output/spark-warehouse\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edd7fe-82a4-4922-ae6f-9916a77a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\")\n",
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\")\n",
    "# Spark Catalog (Metadata) - in-memory/hive\n",
    "\n",
    "spark.conf.get(\"spark.sql.catalogImplementation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a7512-48f6-471e-9186-a4e5aa65e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show databases\n",
    "db = spark.sql(\"show databases\")\n",
    "db.show()\n",
    "\n",
    "\n",
    "spark.sql(\"show tables in default\").show()\n",
    "\n",
    "# Register dataframes are temp views\n",
    "\n",
    "emp.createOrReplaceTempView(\"emp_view\")\n",
    "\n",
    "dept.createOrReplaceTempView(\"dept_view\")\n",
    "# Show tables/view in catalog\n",
    "# View data from table\n",
    "\n",
    "emp_filtered = spark.sql(\"\"\"\n",
    "    select * from emp_view\n",
    "    where department_id = 1\n",
    "\"\"\")\n",
    "emp_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1109e97-3de8-40a9-b947-118f8ccb9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column dob_year and register as temp view\n",
    "\n",
    "emp_temp = spark.sql(\"\"\"\n",
    "    select e.*, date_format(dob, 'yyyy') as dob_year from emp_view e\n",
    "\"\"\")\n",
    "emp_temp.createOrReplaceTempView(\"emp_temp_view\")\n",
    "spark.sql(\"select * from emp_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b15d2-1de8-47d8-a094-07d0a9ca78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join emp and dept - HINTs\n",
    "\n",
    "emp_final = spark.sql(\"\"\"\n",
    "    select /*+ BROADCAST(d) */\n",
    "    e.* , d.department_name\n",
    "    from emp_view e left outer join dept_view d\n",
    "    on e.department_id = d.department_id\n",
    "\"\"\")\n",
    "# Show emp data\n",
    "\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954279a-b103-4f58-ab15-8b42f919aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data as Table\n",
    "\n",
    "emp_final.write.format(\"parquet\").saveAsTable(\"emp_final\")\n",
    "# Read the data from Table\n",
    "\n",
    "emp_new = spark.sql(\"select * from emp_final\")\n",
    "emp_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2a21a-52b3-4f65-9bfc-968343753d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist metadata\n",
    "# Show details of metadata\n",
    "\n",
    "spark.sql(\"describe extended emp_final\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ae485-3cac-41b9-8087-fbd077411539",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read optimizations\n",
    "\n",
    "1. partitioning : \n",
    "    Hive-style partitioning groups similar data in the same directory in storage. \n",
    "    Can be used to partition countries, department etc\n",
    "    \n",
    "\n",
    "\n",
    "2. Z ordering :\n",
    "Z Ordering groups similar data in the same files without creating directories.\n",
    "Z Ordering your data reorganizes the data in storage and allows certain queries to read less data, \n",
    "so they run faster. When your data is appropriately ordered, more files can be skipped.\n",
    "\n",
    "Z Order is particularly important for the ordering of multiple columns. \n",
    "If you only need to order by a single column, then simple sorting suffices. \n",
    "If there are multiple columns, but we always/only query a common prefix of those columns, then hierarchical sorting suffices. \n",
    "Z Ordering is good when querying on one or multiple columns.\n",
    "\n",
    "3. partitioning + z ordering : \n",
    "You can partition a Delta table in storage and Z Order the data within a given partition.\n",
    "\n",
    "For example, you can partition by ingestion_date and Z Order by user_id. \n",
    "This design would be a great way to run queries on user activity over time.\\\n",
    "\n",
    "\n",
    "Delete Optimizations\n",
    "\n",
    "Deletion vectors:\n",
    "Deletion vectors are a storage optimization feature that can be enabled on Delta Lake tables. \n",
    "By default, when a single row in a data file is deleted, the entire Parquet file containing the record must be rewritten.\n",
    "With deletion vectors enabled for the table, some Delta operations use deletion vectors to mark existing rows\n",
    "as removed without rewriting the Parquet file. Subsequent reads on the table resolve current table state by applying\n",
    "the deletions noted by deletion vectors to the most recent table version.\n",
    "\n",
    "Deletion vectors indicate changes to rows as soft-deletes that logically modify existing Parquet data files in the Delta Lake tables.\n",
    "These changes are applied physically when data files are rewritten, as triggered by one of the following events:\n",
    "\n",
    "A DML command with deletion vectors disabled (by a command flag or a table property) is run on the table.\n",
    "An OPTIMIZE command is run on the table.\n",
    "REORG TABLE ... APPLY (PURGE) is run against the table.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Liquid clustering for Delta tables\n",
    "\n",
    "Liquid clustering improves the existing partitioning and ZORDER techniques by simplifying data layout decisions in order to optimize\n",
    "query performance. Liquid clustering provides flexibility to redefine clustering columns without rewriting existing data, \n",
    "allowing data layout to evolve alongside analytic needs over time.\n",
    "\n",
    "\n",
    "What is liquid clustering used for?\n",
    "The following are examples of scenarios that benefit from clustering:\n",
    "\n",
    "Tables often filtered by high cardinality columns.\n",
    "Tables with significant skew in data distribution.\n",
    "Tables that grow quickly and require maintenance and tuning effort.\n",
    "Tables with access patterns that change over time.\n",
    "Tables where a typical partition column could leave the table with too many or too few partitions.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b3fee-477c-47c3-80f7-0f0fcf33cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark Memory\n",
    "\n",
    "1. Spark Memory Distribution\n",
    "2. Storage and Execution Memory\n",
    "3. Why spark runs into OOM errors, even if it can spill data ?\n",
    "4. Understand different reasons for OOM errors.\n",
    "5. See one practical example of OOM error.\n",
    "\n",
    "\n",
    "Spark Memory Management :\n",
    "\n",
    "Driver --> 2 executors --> 512 MB each --> 4 cores\n",
    "\n",
    "2 worker node \n",
    "worker node 1 ---> jvm --> 1 executor --> 2 cores --> 512 mb\n",
    "worker node 2 ---> jvm --> 1 executor --> 2 cores --> 512 mb\n",
    "\n",
    "JVM Memory (JVM heap)\n",
    "\n",
    "On-Heap (default)            Off-Heap(disabled)\n",
    "512mb                           OS\n",
    "managed by jvm\n",
    "\n",
    "512 mb = 89% (available for use 455.69mb) + 11% (GC)\n",
    "\n",
    "455.69 mb = 300 mb (reserved memory for spark internals) + 155.69 mb ( for use) \n",
    "\n",
    "Min Memory to set should be \n",
    "1.5 * 300 mb (reserved memory)\n",
    "400 mb (min memory to set)\n",
    "\n",
    "155.69 mb = 60% (unified spark memory)+ 40% (user memory)  (spark.memory.fraction = 0.6)\n",
    "                93.4mb         +        62.28mb (udf)\n",
    "\n",
    "\n",
    "93.4mb = storage (cache) + execution (hash, agg, shuffle)\n",
    "                                priority\n",
    "\n",
    "1. storage memory can borrow space from execution memory only if execution memory is free\n",
    "2. Execution memory can borrow space from storage memory if its empty \n",
    "and has not reached its storage fraction limit (immune to eviction) spark.memory.storageFraction = 0.5\n",
    "3. Execution memory is used by storage memory and execution needs more memory\n",
    "it can forcefully evict the memory occupied by storage memory at execution side.\n",
    "4. storage needs more memory, it cannot forcefully evict the excess block occupied by\n",
    "Execution memory. It has to wait for execution.\n",
    "\n",
    "\n",
    "OOM errors \n",
    "1. storage memory is full and execution mem is also full \n",
    "and in conf memory mode is MEMORY_ONLY not MEMORY_AND_DISK is set \n",
    "in this scenario we get OOM error\n",
    "\n",
    "memory --> java objects (deserialized) disk --> byte stream (serialized) --> resource and time\n",
    "\n",
    "2. execution memory full\n",
    "assume \n",
    "Execution memory = 40 mb and 2 cores\n",
    "20mb /core\n",
    "\n",
    "first core has data of 25mb then it wont fit in execution memory spillage to disk\n",
    "other core is having 5 mb record this is known as data skewness\n",
    "\n",
    "lets say there are 3 records after deserialization size is 25 mb each\n",
    "then one record itself wont fit in memory\n",
    "as it cannot split a record\n",
    "\n",
    "3. wide shuffle scenario \n",
    "\n",
    "4. broadcast variable\n",
    "\n",
    "5. data explosion - cross join, explode, compressed data when u uncompress it\n",
    "\n",
    "6. compress\n",
    "\n",
    "7. GC is taking more time to free up in that case we get OOM\n",
    "\n",
    "Off Heap Memory\n",
    "\n",
    "need to enable to use but GC need to handle explicitly\n",
    "\n",
    "'''         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
