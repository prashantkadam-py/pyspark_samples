{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd6d6195-ee60-4a38-873d-0cd41872706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### spark session\n",
    "\n",
    "# Ref : https://github.com/subhamkharwal/pyspark-zero-to-hero/blob/master/05_sort_union_aggregation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c7a731-a621-4a75-bb68-f0d0841dbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Spark Session\")\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c42287a-f2b9-4850-b4ff-b8050650a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a674bfa4fa0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #print session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a006002-8a29-4525-a150-71c91849f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe\n",
    "data = [[1, \"ABC\", 2000], [2, \"DEF\", 4000], [3, \"GGGG\", 3000], [4, 'HHHH', 7000], [5, 'IIIII', 4500]]\n",
    "df = spark.createDataFrame(data=data, schema=\"id int, name string, salary int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9596ce7e-14b9-4f5e-bad4-5782a545e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a68d94f0-9a12-42f8-9c81-1fb791735d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  4|HHHH|  7000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter\n",
    "df.where(\"salary > 5000\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4217b088-5655-4a01-b8b1-f10594c8ddd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() # num of partti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcb17df2-d751-46f8-b543-b08ab0d4a35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/prashant/pivot/personal/tech/pyspark_samples/test.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#action\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/prashant/pivot/personal/tech/pyspark_samples/test.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df.write.format('csv').save(\"test.csv\") #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a63e6c3-8d20-4176-baa8-e4ecf5472ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a674bfa4fa0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Active Session and Rename\n",
    "\n",
    "spark_new = spark.getActiveSession()\n",
    "spark_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6245f8fe-a782-45a9-becf-5fde2b292b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "277ee3cd-9ea5-4e7a-a8fd-fd34b71e541d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('salary', IntegerType(), True)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdca758e-bc33-4c25-b8f8-ac1aa512e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a91d7c3f-8468-4872-8f46-68640ea180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark_new.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5f0c37-965e-4e65-b305-5ec71ff16cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2059ee3a-e4e4-408a-9624-8052b2384883",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"id int, name string, salary int\"\n",
    "df1 = spark_new.createDataFrame(data=data, schema=schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df1df41-2e52-41f7-afec-71861f801c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37aaa6f8-1087-4c20-8357-e1769568bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df1.select(col(\"id\"), df1.name, expr(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69c325e6-3729-48ba-90cc-40adbafc24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.selectExpr(\"cast(id as int) as id\", \"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ca9c34b-0d62-4254-a92b-582738ab4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"tax\", col(\"salary\") * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac45b677-d1e8-422e-9e87-736153a6557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary|  taxx|\n",
      "+---+-----+------+------+\n",
      "|  1|  ABC|  2000| 400.0|\n",
      "|  2|  DEF|  4000| 800.0|\n",
      "|  3| GGGG|  3000| 600.0|\n",
      "|  4| HHHH|  7000|1400.0|\n",
      "|  5|IIIII|  4500| 900.0|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumnRenamed(\"tax\", \"taxx\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7db3e2cb-413d-4634-abdf-7dfe5e9dcde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  ABC|  2000|\n",
      "|  2|  DEF|  4000|\n",
      "|  3| GGGG|  3000|\n",
      "|  4| HHHH|  7000|\n",
      "|  5|IIIII|  4500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.drop(\"tax\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8aae17c-a355-4a23-a679-8d71193fa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "columns = {\n",
    "    \"col1\" : lit(1),\n",
    "    \"tax\" : expr(\"salary\") * 0.2,\n",
    "    \"col2\" : lit(\"col2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2bad62f-275b-4c6d-945a-52368a14bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+------+----+\n",
      "| id| name|salary| bonus|col1|   tax|col2|\n",
      "+---+-----+------+------+----+------+----+\n",
      "|  1|  ABC|  2000| 400.0|   1| 400.0|col2|\n",
      "|  2|  DEF|  4000| 800.0|   1| 800.0|col2|\n",
      "|  3| GGGG|  3000| 600.0|   1| 600.0|col2|\n",
      "|  4| HHHH|  7000|1050.0|   1|1400.0|col2|\n",
      "|  5|IIIII|  4500| 675.0|   1| 900.0|col2|\n",
      "+---+-----+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumns(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b3c1a9f-9a4b-4dc5-b9dd-01b8eb5fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2a56313-559a-4dbd-a756-ea3c49d9cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"bonus\", \n",
    "               when(col(\"salary\") <= 4000, col(\"salary\") * 0.2)\n",
    "               .when(col(\"salary\")>4000, col(\"salary\") * 0.15)\n",
    "               .otherwise(col(\"salary\")*1)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e35be6e3-4b07-448b-81d5-936819c413cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary| bonus|\n",
      "+---+-----+------+------+\n",
      "|  1|  ABC|  2000| 400.0|\n",
      "|  2|  DEF|  4000| 800.0|\n",
      "|  3| GGGG|  3000| 600.0|\n",
      "|  4| HHHH|  7000|1050.0|\n",
      "|  5|IIIII|  4500| 675.0|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c617f904-2ac5-4687-ab8e-53a66ff05f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+\n",
      "| id| name|salary| bonus|   tax|\n",
      "+---+-----+------+------+------+\n",
      "|  1|  ABC|  2000| 400.0| 200.0|\n",
      "|  2|  DEF|  4000| 800.0| 400.0|\n",
      "|  3| GGGG|  3000| 600.0| 300.0|\n",
      "|  4| HHHH|  7000|1050.0|1400.0|\n",
      "|  5|IIIII|  4500| 675.0| 900.0|\n",
      "+---+-----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"tax\", expr(\"case when salary <= 4000 then (salary * 0.1) when salary > 4000 then (salary * 0.2) else 0 end\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f86677-3d7c-4a59-b160-51d9044ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION \n",
    "'''\n",
    "Column names, datatypes and sequence of columns of both the dataframes should be same.\n",
    "union all - keeps duplicate rows\n",
    "union - keeps distinct records\n",
    "unionByName - works with diff column seq but number of cols and datatype should be same\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2e35213-1135-41f3-b1b2-a969834aa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data_1 = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"]\n",
    "]\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e32bf011-f200-4e49-89a8-df01e82514a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df1 = spark_new.createDataFrame(data=emp_data_1, schema = emp_schema)\n",
    "emp_df2 = spark_new.createDataFrame(data=emp_data_2, schema = emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfeed930-6a3c-42b9-af7c-e60bdfde1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = emp_df1.union(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "abe7b43a-7f7b-4930-865a-412f47e2167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3683b42a-30f5-4931-b564-2110f2450938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "emp_df_new = emp_df.withColumn(\"new_gender\", coalesce(\"gender\", lit(\"O\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "758835be-3953-4d16-8a68-8dc371509a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|      Male|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|    Female|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|      Male|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|    Female|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|      Male|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|    Female|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|      Male|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|    Female|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|      Male|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|    Female|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|      Male|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|    Female|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|      Male|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|    Female|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|      Male|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|    Female|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|      Male|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|          |\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|      Male|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|    Female|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09da2c1a-b88b-4209-bd7c-32cd408be235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|department_id|count(employee_id)|\n",
      "+-------------+------------------+\n",
      "|          101|                 3|\n",
      "|          102|                 4|\n",
      "|          103|                 4|\n",
      "|          104|                 3|\n",
      "|          105|                 2|\n",
      "|          106|                 2|\n",
      "|          107|                 2|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "emp_df_new.groupBy(\"department_id\").agg(count(\"employee_id\")).alias(\"employee_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70503dd-8303-4f54-b752-5ad9bf49234e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8ad93-1895-4e06-8fa4-43bbcab55af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8029a8b-fad7-4424-8e8b-71cef6e57c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3190e-56c4-46f9-935f-223454c34485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5771fc-8b37-4df6-aa3a-9366140da098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef69be-3a81-4b47-8357-a327842098df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb906f-3e12-4ee8-a6a1-0b185f488525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9769a5f-904b-4fc8-98fc-526775ef9b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221ad23-d849-497b-9d95-3cb3171c1a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a03d6-df78-4ad3-840a-256ec8fad466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284733bd-e853-4a1b-a651-ea42236e6103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b406afe-a980-4e07-b107-cc64a4820aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
